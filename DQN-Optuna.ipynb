{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from pogema import GridConfig\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "grid_config = GridConfig(\n",
    "    size=8,\n",
    "    density=0.3,\n",
    "    num_agents=1,\n",
    "    max_episode_steps=30\n",
    ")\n",
    "\n",
    "env = gym.make(\"Pogema-v0\",grid_config=grid_config)\n",
    "\n",
    "# dqn_model = DQN(\n",
    "#     \"MlpPolicy\",\n",
    "#     env,\n",
    "#     verbose=1,\n",
    "#     train_freq=16,\n",
    "#     gradient_steps=8, # gradient steps to do after each rollout\n",
    "#     gamma=0.99, # discount factor\n",
    "#     exploration_fraction=0.4, \n",
    "#     exploration_final_eps=0.07,\n",
    "#     target_update_interval=600, # update the target network afte fixed number of steps\n",
    "#     learning_starts=1000, # how many steps of the model to collect transitions for before learning starts\n",
    "#     buffer_size=10000, # size of the replay buffer\n",
    "#     batch_size=128, # Minibatch size for each gradient update\n",
    "#     learning_rate=4e-3,\n",
    "#     policy_kwargs=dict(net_arch=[256, 256]),\n",
    "#     seed=42,\n",
    "#     tensorboard_log=\"./tensorboard\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 13:27:07,604] A new study created in memory with name: no-name-0e14323e-891f-489c-bce3-9c6923cc87a0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-02 13:27:07,608] Trial 0 failed with parameters: {'gamma': 0.024419338679323326, 'max_grad_norm': 0.894498017625567, 'exploration_fraction': 0.19738703321857887, 'lr': 3.608344357670346e-05, 'net_arch': 'small'} because of the following error: TypeError('empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_36830/2458671121.py\", line 97, in objective\n",
      "    model = DQN(**kwargs)\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py\", line 141, in __init__\n",
      "    self._setup_model()\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py\", line 144, in _setup_model\n",
      "    super()._setup_model()\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 199, in _setup_model\n",
      "    self.policy = self.policy_class(  # pytype:disable=not-instantiable\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py\", line 151, in __init__\n",
      "    self._build(lr_schedule)\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py\", line 163, in _build\n",
      "    self.q_net = self.make_q_net()\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py\", line 178, in make_q_net\n",
      "    return QNetwork(**net_args).to(self.device)\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py\", line 56, in __init__\n",
      "    q_net = create_mlp(self.features_dim, action_dim, self.net_arch, self.activation_fn)\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py\", line 135, in create_mlp\n",
      "    modules = [nn.Linear(input_dim, net_arch[0], bias=with_bias), activation_fn()]\n",
      "  File \"/home/bernard/miniconda3/envs/rlenv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 96, in __init__\n",
      "    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
      "TypeError: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n",
      " * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      " * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      "\n",
      "[W 2023-11-02 13:27:07,609] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'Pogema-v0'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(sampler\u001b[39m=\u001b[39msampler, pruner\u001b[39m=\u001b[39mpruner, direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mN_TRIALS, timeout\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m kwargs\u001b[39m.\u001b[39mupdate(sample_dqn_params(trial))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39m# Create the RL model.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# Create env used for evaluation.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/bernard/nus/cs5446/cs5446_pogs-1/DQN-Optuna.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m eval_env \u001b[39m=\u001b[39m Monitor(env)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:141\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[0;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_rate \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m _init_setup_model:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_model()\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:144\u001b[0m, in \u001b[0;36mDQN._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_model()\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_aliases()\n\u001b[1;32m    146\u001b[0m     \u001b[39m# Copy running stats, see GH issue #996\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:199\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[39m\"\u001b[39m\u001b[39menv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\n\u001b[1;32m    189\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer_class(\n\u001b[1;32m    190\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size,\n\u001b[1;32m    191\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mreplay_buffer_kwargs,  \u001b[39m# pytype:disable=wrong-keyword-args\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_class(  \u001b[39m# pytype:disable=not-instantiable\u001b[39;49;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space,\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_space,\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_schedule,\n\u001b[1;32m    203\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_kwargs,  \u001b[39m# pytype:disable=not-instantiable\u001b[39;49;00m\n\u001b[1;32m    204\u001b[0m )\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    207\u001b[0m \u001b[39m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py:151\u001b[0m, in \u001b[0;36mDQNPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn \u001b[39m=\u001b[39m activation_fn\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_args \u001b[39m=\u001b[39m {\n\u001b[1;32m    144\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mobservation_space\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[1;32m    145\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maction_space\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnormalize_images\u001b[39m\u001b[39m\"\u001b[39m: normalize_images,\n\u001b[1;32m    149\u001b[0m }\n\u001b[0;32m--> 151\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build(lr_schedule)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py:163\u001b[0m, in \u001b[0;36mDQNPolicy._build\u001b[0;34m(self, lr_schedule)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build\u001b[39m(\u001b[39mself\u001b[39m, lr_schedule: Schedule) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    Create the network and the optimizer.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39m        lr_schedule(1) is the initial learning rate\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_q_net()\n\u001b[1;32m    164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_q_net()\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net_target\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net\u001b[39m.\u001b[39mstate_dict())\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py:178\u001b[0m, in \u001b[0;36mDQNPolicy.make_q_net\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_q_net\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m QNetwork:\n\u001b[1;32m    176\u001b[0m     \u001b[39m# Make sure we always have separate networks for features extractors etc\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     net_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_features_extractor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_args, features_extractor\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m QNetwork(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnet_args)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py:56\u001b[0m, in \u001b[0;36mQNetwork.__init__\u001b[0;34m(self, observation_space, action_space, features_extractor, features_dim, net_arch, activation_fn, normalize_images)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_dim \u001b[39m=\u001b[39m features_dim\n\u001b[1;32m     55\u001b[0m action_dim \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn)  \u001b[39m# number of actions\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m q_net \u001b[39m=\u001b[39m create_mlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_dim, action_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet_arch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation_fn)\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39mq_net)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:135\u001b[0m, in \u001b[0;36mcreate_mlp\u001b[0;34m(input_dim, output_dim, net_arch, activation_fn, squash_output, with_bias)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mCreate a multi layer perceptron (MLP), which is\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39ma collection of fully-connected layers each followed by an activation function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(net_arch) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m     modules \u001b[39m=\u001b[39m [nn\u001b[39m.\u001b[39;49mLinear(input_dim, net_arch[\u001b[39m0\u001b[39;49m], bias\u001b[39m=\u001b[39;49mwith_bias), activation_fn()]\n\u001b[1;32m    136\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     modules \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from typing import Dict\n",
    "\n",
    "import gymnasium\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 5\n",
    "N_EVALUATIONS = 2\n",
    "N_TIMESTEPS = int(1.2e5)\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 3\n",
    "\n",
    "\n",
    "ENV_ID = \"Pogema-v0\"\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"verbose\": 1,\n",
    "    \"env\": ENV_ID\n",
    "}\n",
    "\n",
    "def sample_dqn_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for DQN hyperparameters.\"\"\"\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    exploration_fraction = trial.suggest_float(\"exploration_fraction\", 0.1, 0.5, log=True)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "\n",
    "    # Display true values.\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [64], \"vf\": [64]} if net_arch == \"tiny\" else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"exploration_fraction\": exploration_fraction,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"Callback used for evaluating and reporting a trial.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gymnasium.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need.\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    # Sample hyperparameters.\n",
    "    kwargs.update(sample_dqn_params(trial))\n",
    "    # Create the RL model.\n",
    "    model = DQN(**kwargs)\n",
    "    # Create env used for evaluation.\n",
    "    eval_env = Monitor(env)\n",
    "    # Create the callback that will periodically evaluate and report the performance.\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN.\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory.\n",
    "        model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed.\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set pytorch num threads to 1 for faster training.\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "    # Do not prune before 1/3 of the max budget is used.\n",
    "    pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\n",
    "\n",
    "    study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, timeout=600)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    print(\"  User attrs:\")\n",
    "    for key, value in trial.user_attrs.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
